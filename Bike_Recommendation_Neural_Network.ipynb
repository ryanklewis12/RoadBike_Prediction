{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price  average_rating  total_ratings                   description   label\n",
      "0   1000             4.5            200       Lightweight and durable  Bike A\n",
      "1   1200             4.0            150       Affordable and reliable  Bike B\n",
      "2    900             4.7            300  High performance and comfort  Bike C\n",
      "3   1100             4.3            180    Lightweight and affordable  Bike A\n",
      "4   1050             4.6            220  Durable and high performance  Bike B\n",
      "5    950             4.2            170      Comfortable and reliable  Bike C\n",
      "[0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'price': [1000, 1200, 900, 1100, 1050, 950],\n",
    "    'average_rating': [4.5, 4.0, 4.7, 4.3, 4.6, 4.2],\n",
    "    'total_ratings': [200, 150, 300, 180, 220, 170],\n",
    "    'description': [\"Lightweight and durable\", \"Affordable and reliable\", \"High performance and comfort\", \"Lightweight and affordable\", \"Durable and high performance\", \"Comfortable and reliable\"],\n",
    "    'label': [\"Bike A\", \"Bike B\", \"Bike C\", \"Bike A\", \"Bike B\", \"Bike C\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Tokenize and pad text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['description'])\n",
    "sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "max_text_length = max(len(seq) for seq in sequences)\n",
    "text_data_padded = pad_sequences(sequences, maxlen=max_text_length)\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_data = label_encoder.fit_transform(df['label'])\n",
    "num_bikes = len(label_encoder.classes_)\n",
    "\n",
    "print(label_data)\n",
    "\n",
    "# Extract numerical data\n",
    "numerical_data = df[['price', 'average_rating', 'total_ratings']].values\n",
    "\n",
    "# Numerical Input\n",
    "numerical_input = Input(shape=(3,), name='numerical_input')\n",
    "x_num = Dense(64, activation='relu')(numerical_input)\n",
    "x_num = Dense(32, activation='relu')(x_num)\n",
    "\n",
    "# Text Input (Description)\n",
    "vocab_size = 10000  \n",
    "text_input = Input(shape=(max_text_length,), name='text_input')\n",
    "x_text = Embedding(input_dim=vocab_size, output_dim=128)(text_input)\n",
    "x_text = LSTM(128)(x_text)\n",
    "\n",
    "# Concatenate\n",
    "x = concatenate([x_num, x_text])\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(num_bikes, activation='softmax')(x)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=[numerical_input, text_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "numerical_train, numerical_val, text_train, text_val, label_train, label_val = train_test_split(\n",
    "    numerical_data, text_data_padded, label_data, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.5000 - loss: 10.7024 - val_accuracy: 0.0000e+00 - val_loss: 19.8272\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.5000 - loss: 9.9271 - val_accuracy: 0.0000e+00 - val_loss: 15.0370\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.5000 - loss: 7.8480 - val_accuracy: 0.0000e+00 - val_loss: 8.0499\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.5000 - loss: 4.6214 - val_accuracy: 0.5000 - val_loss: 5.2398\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.2500 - loss: 4.4877 - val_accuracy: 0.5000 - val_loss: 5.7082\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.2500 - loss: 7.3768 - val_accuracy: 0.5000 - val_loss: 4.4575\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.2500 - loss: 7.5259 - val_accuracy: 0.5000 - val_loss: 1.7704\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.2500 - loss: 5.2828 - val_accuracy: 0.5000 - val_loss: 1.1512\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.2500 - loss: 4.9046 - val_accuracy: 0.5000 - val_loss: 3.0693\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2500 - loss: 4.9283 - val_accuracy: 0.5000 - val_loss: 2.8266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 2.8266\n",
      "Validation Loss: 2.8265931606292725\n",
      "Validation Accuracy: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Recommended Bike: Bike A\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [numerical_train, text_train], label_train,\n",
    "    validation_data=([numerical_val, text_val], label_val),\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([numerical_val, text_val], label_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# Example user input\n",
    "user_numerical_input = np.array([[1100, 4.3, 180]])\n",
    "user_text_input = [\"Lightweight and affordable\"]\n",
    "user_text_sequences = tokenizer.texts_to_sequences(user_text_input)\n",
    "user_text_padded = pad_sequences(user_text_sequences, maxlen=max_text_length)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict([user_numerical_input, user_text_padded])\n",
    "predicted_bike_index = np.argmax(predictions, axis=1)\n",
    "predicted_bike_name = label_encoder.inverse_transform(predicted_bike_index)\n",
    "\n",
    "print(f'Recommended Bike: {predicted_bike_name[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
